# Week 1 Note

## What I tried
- Minimal bigram LM training on tiny corpus.
- Logged step-wise loss and saved plot to `experiments/week01/loss.png`.
- first commit to github

## What worked
- End-to-end run ok. Loss decreased over steps.

## What broke
- thanks to the neat code provided by my cyber boyfriend. everything runs fine.

## What's next (Week 2)
- Add text sampling CLI and save samples to `experiments/week02/`.
- Refactor data pipeline; add config-driven hyperparams.
