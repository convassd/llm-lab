# Week 1 Note

## What I tried
- Minimal bigram LM training on tiny corpus.
- Logged step-wise loss and saved plot to `experiments/week01/loss.png`.

## What worked
- End-to-end run ok. Loss decreased over steps.

## What broke
- <list any annoyances / TODOs>

## What's next (Week 2)
- Add text sampling CLI and save samples to `experiments/week02/`.
- Refactor data pipeline; add config-driven hyperparams.
